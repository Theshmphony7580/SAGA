# SAGA (Semantic & Autonomous Generative Analytics)

A production-ready, AI-driven data analytics platform built with FastAPI (backend) and Streamlit (frontend). Upload datasets, automatically profile and clean data, generate insights, ask natural language questions, and create visualizationsâ€”all through an intuitive interface.

---

## ğŸŒŸ Features

### Core Capabilities
- **ğŸ“¤ Smart Upload**: Support for CSV and XLSX files with automatic validation and encoding detection
- **ğŸ” Intelligent Profiling**: ML-powered dataset analysis using `ydata-profiling`
- **ğŸ§¹ Auto-Cleaning**: Automatic missing value imputation, outlier detection, and data normalization
- **ğŸ’¡ AI Insights**: Correlation analysis, statistical summaries, categorical breakdowns, and anomaly detection
- **ğŸ’¬ Natural Language Queries (NLQ)**: Ask questions in plain English, get SQL results instantly
- **ğŸ“Š Dynamic Visualizations**: Interactive charts with automatic type recommendations
- **ğŸ“ Report Generation**: Export analysis reports in HTML or Excel format

### Technical Highlights
- **Database-First Architecture**: SQLite-backed persistence with full ACID compliance
- **Production-Ready**: Comprehensive logging, error handling, CORS security, and health checks
- **Modular Design**: Clean separation of concerns across API, ML, database, and utility layers
- **Type-Safe**: Pydantic models for request/response validation
- **Extensible**: Plugin-ready architecture for custom ML models and data sources

---

## ğŸ—ï¸ Architecture

```
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/              # FastAPI route handlers
â”‚   â”‚   â”œâ”€â”€ upload.py     # Dataset upload endpoint
â”‚   â”‚   â”œâ”€â”€ profile.py    # ML-powered profiling
â”‚   â”‚   â”œâ”€â”€ clean.py      # Data cleaning pipeline
â”‚   â”‚   â”œâ”€â”€ insights.py   # Statistical analysis
â”‚   â”‚   â”œâ”€â”€ nlq.py        # Natural language queries
â”‚   â”‚   â”œâ”€â”€ charts.py     # Visualization recommendations
â”‚   â”‚   â””â”€â”€ report.py     # Report generation
â”‚   â”œâ”€â”€ ml/               # Machine learning modules
â”‚   â”‚   â”œâ”€â”€ auto_profiler.py    # ydata-profiling integration
â”‚   â”‚   â”œâ”€â”€ cleaning.py         # Imputation & outlier removal
â”‚   â”‚   â”œâ”€â”€ insights_engine.py  # Statistical insights
â”‚   â”‚   â”œâ”€â”€ nlq_engine.py       # NLQ â†’ SQL conversion
â”‚   â”‚   â””â”€â”€ chart_rules.py      # Chart type logic
â”‚   â”œâ”€â”€ database/         # SQLite utilities
â”‚   â”‚   â”œâ”€â”€ init_db.py    # Schema initialization
â”‚   â”‚   â””â”€â”€ utils.py      # CRUD operations
â”‚   â”œâ”€â”€ utils/            # Helper functions
â”‚   â”‚   â”œâ”€â”€ file_utils.py       # File I/O operations
â”‚   â”‚   â”œâ”€â”€ data_utils.py       # CSV parsing & validation
â”‚   â”‚   â””â”€â”€ security.py         # Input sanitization
â”‚   â”œâ”€â”€ config.py         # Configuration constants
â”‚   â””â”€â”€ main.py           # FastAPI application
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py            # Streamlit UI
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ db/               # SQLite database
â”‚   â””â”€â”€ datasets/         # Uploaded files (temporary)
â”œâ”€â”€ reports/              # Generated reports
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- pip or conda

### Installation

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd ai-data-analytics-assistant
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Initialize the database**
   ```bash
   python -m backend.database.init_db
   ```

### Running Locally

**Option 1: Separate terminals**

Terminal 1 (Backend):
```bash
uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000
```

Terminal 2 (Frontend):
```bash
streamlit run frontend/app.py
```

**Option 2: Using the entry point**
```bash
python -m backend  # Starts backend on port 8000
streamlit run frontend/app.py  # In another terminal
```

**Access the application:**
- Frontend: http://localhost:8501
- Backend API Docs: http://localhost:8000/docs

---

## ğŸ“– Usage Guide

### 1. Upload Dataset
- Click "Upload CSV or XLSX"
- Select your file (max 200 MB)
- Click "Upload" to process

### 2. Profile Your Data
- Click "Generate Profile"
- View summary statistics, column types, missing values, and correlations

### 3. Clean Data
- Click "Run Cleaning"
- Automatically handles:
  - Missing values (mean/mode imputation)
  - Outliers (IQR-based removal)
  - Data type inference

### 4. Generate Insights
- Click "Generate Insights"
- Explore:
  - Numeric summaries (mean, median, std, min, max, quartiles)
  - Correlation matrices
  - Category-wise breakdowns
  - Top/bottom extreme values

### 5. Ask Questions (NLQ)
Supported queries:
- "Show first 10 rows"
- "What are the columns?"
- "Display last 5 rows"
- More patterns in `backend/ml/nlq_engine.py`

### 6. Create Charts
- Select X and Y axes
- Choose chart type (line, bar, scatter)
- Click "Generate Chart"

### 7. Export Reports
- Select sections to include
- Choose format (HTML or Excel)
- Download from `reports/` directory

---

## ğŸ”§ Configuration

### Environment Variables
Create a `.env` file (optional):
```bash
BACKEND_URL=http://127.0.0.1:8000  # For frontend connection
MAX_FILE_SIZE_MB=200
```

### Backend Config (`backend/config.py`)
```python
MAX_FILE_SIZE_MB = 200
ALLOWED_EXTENSIONS = [".csv", ".xlsx"]
ALLOWED_ORIGINS = ["http://localhost:8501"]
```

---

## ğŸ§ª API Reference

### Base URL: `/v1/api`

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/upload` | POST | Upload CSV/XLSX file |
| `/profile` | GET | Generate data profile |
| `/clean/{dataset_id}` | POST | Clean dataset |
| `/insights/{dataset_id}` | POST | Generate insights |
| `/nlq/run` | POST | Execute NLQ query |
| `/columns` | GET | List dataset columns |
| `/charts/plot` | POST | Generate chart data |
| `/report/export` | POST | Export analysis report |
| `/datasets/{dataset_id}` | GET | Fetch dataset metadata |
| `/datasets/{dataset_id}` | DELETE | Delete dataset |
| `/health` | GET | Health check |

**Example Request (cURL):**
```bash
# Upload file
curl -X POST "http://localhost:8000/v1/api/upload" \
  -F "file=@data.csv"

# Run NLQ
curl -X POST "http://localhost:8000/v1/api/nlq/run" \
  -H "Content-Type: application/json" \
  -d '{"dataset_id": "abc-123", "question": "show first 5 rows"}'
```

---

## ğŸ› ï¸ Development

### Running Tests
```bash
pytest tests/  # (Tests directory to be created)
```

### Code Style
```bash
black backend/ frontend/
isort backend/ frontend/
```

### Database Schema
```sql
CREATE TABLE datasets (
    id TEXT PRIMARY KEY,
    filename TEXT NOT NULL,
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    table_name TEXT NOT NULL,
    is_cleaned BOOLEAN DEFAULT FALSE,
    source_dataset_id TEXT,
    FOREIGN KEY (source_dataset_id) REFERENCES datasets(id)
);
```

---

## ğŸš¢ Deployment

### Docker (Recommended)
```dockerfile
# Dockerfile (to be created)
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Cloud Platforms
- **Render**: Connect GitHub repo â†’ Auto-deploy
- **Railway**: One-click FastAPI + SQLite deployment
- **Google Cloud Run**: Containerized deployment

---

## ğŸ“š Tech Stack

### Backend
- **FastAPI** - High-performance API framework
- **Pandas** - Data manipulation
- **ydata-profiling** - Advanced dataset profiling
- **SQLite** - Embedded database
- **Loguru** - Production logging

### Frontend
- **Streamlit** - Interactive web interface
- **Plotly** - Interactive visualizations
- **Requests** - HTTP client

### ML/Analytics
- **NumPy** - Numerical computing
- **SciPy** - Statistical analysis
- **Scikit-learn** - Outlier detection (future: IsolationForest)

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed for local development use. For production deployment, please review and add an appropriate license (MIT, Apache 2.0, etc.).

---

## ğŸ› Troubleshooting

### Common Issues

**Issue**: "Dataset not found"
- **Solution**: Verify `dataset_id` matches UUID format, not table name

**Issue**: CSV parsing fails
- **Solution**: Check encoding (UTF-8 recommended), ensure consistent column counts

**Issue**: Backend unreachable
- **Solution**: Confirm `BACKEND_URL` in frontend matches running backend port

**Issue**: File upload fails (413 error)
- **Solution**: File exceeds `MAX_FILE_SIZE_MB` (default 200MB)

---

## ğŸ“ Support

- **Issues**: [GitHub Issues](your-repo-url/issues)
- **Discussions**: [GitHub Discussions](your-repo-url/discussions)
- **Email**: your-email@example.com

---